{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>jQuery(function() {if (jQuery(\"body.notebook_app\").length == 0) { jQuery(\".input_area\").toggle(); jQuery(\".prompt\").toggle();}});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onclick=\"jQuery('.input_area').toggle(); jQuery('.prompt').toggle();\">Toggle code</button>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "from IPython.display import HTML\n",
    "import IPython.core.display as di # Example: di.display_html('<h3>%s:</h3>' % str, raw=True)\n",
    "\n",
    "# This line will hide code by default when the notebook is exported as HTML\n",
    "di.display_html('<script>jQuery(function() {if (jQuery(\"body.notebook_app\").length == 0) { jQuery(\".input_area\").toggle(); jQuery(\".prompt\").toggle();}});</script>', raw=True)\n",
    "\n",
    "# This line will add a button to toggle visibility of code blocks, for use with the HTML export version\n",
    "di.display_html('''<button onclick=\"jQuery('.input_area').toggle(); jQuery('.prompt').toggle();\">Toggle code</button>''', raw=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receipt.ID\n",
    "### Hierarchical text item classification\n",
    "Taxonomic classification, categorize items according to a pre-defined taxonomy. The goal is to assign one or more categories in the taxonomy to an item. It is a multi-class **and** multi-label classification problem with hierarchical relationships between each node in the tree.\n",
    "\n",
    "#### Items\n",
    "- Items come from a wide range for categories like Produce, Meat, Beverage, Supplies. \n",
    "- Example item to category mapping:\n",
    "\n",
    "\n",
    "|item|mapping|\n",
    "|---|---|\n",
    "|Kale  | \"Food/Produce/Kale\"  |\n",
    "|Vinegar white wine 50 grain  | \"Food/Dry-Grocery/Vinegars/White Wine Vinegar\"  |\n",
    "|Imported nat flank steak  | \"Food/Meats/Beef/Flank Steak\"  |\n",
    "\n",
    "To solve this problem, I will undertake the following course of action:\n",
    "1. Explore the dataset\n",
    "    - Explore the dataset to ensure its integrity and understand the context. \n",
    "2. Identify features that may be used. \n",
    "    - If possible, engineer features that might provide greater discrimination.\n",
    "3. Build k independent *text-based* classifiers for the text-based features and feed the output from these classifiers into the next layer classifier which takes in the other features. Explore a couple of classifiers that might be well suited for the problem at hand.\n",
    "    - Decision Trees\n",
    "    - SVM\n",
    "    - AdaBoost\n",
    "    - Random Forest\n",
    "4.  Select appropriate classifier based on evaluation metric and tune it for optimality.\n",
    "\n",
    "In this notebook I do processes 3 and 4.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "sys.path.append('tools/')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Graphing Libraries\n",
    "import matplotlib.pyplot as pyplt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")  \n",
    "\n",
    "# Use CPickle if available\n",
    "try:\n",
    "   import cPickle as pickle\n",
    "except:\n",
    "   import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in a data file with 127108 datapoints\n"
     ]
    }
   ],
   "source": [
    "dataPath = '/Users/omojumiller/mycode/insight/PlateIQ/'\n",
    "df = pd.read_pickle(dataPath+'data/df_data_vectors.dat')\n",
    "df_catergory_lookup = pd.read_pickle(dataPath+'data/data_category_lookup.dat')\n",
    "print ('Read in a data file with {0} datapoints'.format(len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define helper functions\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def shuffle_split_data(X, y):\n",
    "    \"\"\" Shuffles and splits data into 75% training and 25% testing subsets,\n",
    "        then returns the training and testing subsets. \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=num_train, random_state=42)\n",
    "    \n",
    "    # Return the training and testing data subsets\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    \"\"\"Transforms lists of feature-value mappings to vectors.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    object : word2vec model \n",
    "    \"\"\"\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = len(word2vec.itervalues().next())\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        return self \n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec] \n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_categories(level, num_data = 1000):\n",
    "    \"\"\"Gets categories which at least num_data datapoints.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    level : column name\n",
    "    num_data : int\n",
    "    \"\"\"    \n",
    "    the_categories = {}\n",
    "    counts = df.groupby(level).size()\n",
    "    total = df.groupby(level).size().sum()\n",
    "    \n",
    "    for i in counts.index:\n",
    "        if counts[i] > num_data:\n",
    "            the_categories[int(i)] = counts[i]\n",
    "\n",
    "\n",
    "    print(level, 'has', len(the_categories), 'categories with enough data.')  \n",
    "\n",
    "    for key in the_categories:\n",
    "        find_name = df_catergory_lookup.category_id == key\n",
    "        name = df_catergory_lookup[find_name].category_name\n",
    "        the_index = name.index[0]\n",
    "\n",
    "    return the_categories, total\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current approach\n",
    "The current approach used focuses exclusively on the item's name, for example, a data point in the dataset would have an object's name as \"mary's organic fryers\" or \"organic baby spinach.\" The first challenge with this approach lay in the fact that the item label was quite short, roughly about two to eight words.  Further, when modifiers like *gluten free*, *organic*, or *pesticide free* wherein the item's label, this added a layer of misinformation causing items like *organic milk* and *organic beer* to be classified in the same class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## My Approach\n",
    "I took an entirely different approach. I got inspiration from the approach that Google, [YouTube](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36411.pdf) used in organizing videos and decided to shift the unit of analysis from item *name* to the *categories* themselves. My approach combines information from both the text-based labels as well as the item's metadata.\n",
    "\n",
    "This method achieves two crucial things. First, by focusing on individual categories, each time a new category of item is added to the restaurant domain, instead of having to retrain the classifier on the entire dataset, all we have to do is gather enough data for that category, and train a classifier for it. This way, the approach can scale beautifully as the taxonomy grows. Second, moving the unit of analysis from text labels to categories, it becomes easier to correctly separate \"organic cream\" and \"organic beer.\" \n",
    "\n",
    "I chose to discard categories that had less than 300 datapoints. As the datapoints in the category grows, those categories can then been trained individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level_0 has 6 categories with enough data.\n"
     ]
    }
   ],
   "source": [
    "the_level = {}\n",
    "num_data = 300\n",
    "\n",
    "level = 'level_0'\n",
    "the_level[level], total = get_categories(level, num_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level_1 has 19 categories with enough data.\n"
     ]
    }
   ],
   "source": [
    "level = 'level_1'\n",
    "the_level[level], total = get_categories(level, num_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level_2 has 65 categories with enough data.\n"
     ]
    }
   ],
   "source": [
    "level = 'level_2'\n",
    "df.loc[:,('label')] = df.loc[:, level]\n",
    "the_level[level], total = get_categories(level, num_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level_3 has 42 categories with enough data.\n"
     ]
    }
   ],
   "source": [
    "level = 'level_3'\n",
    "df.loc[:,('label')] = df.loc[:, level]\n",
    "the_level[level], total = get_categories(level, num_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level_4 has 9 categories with enough data.\n"
     ]
    }
   ],
   "source": [
    "level = 'level_4'\n",
    "df.loc[:,('label')] = df.loc[:, level]\n",
    "the_level[level], total = get_categories(level, num_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 141 categories in all\n"
     ]
    }
   ],
   "source": [
    "k_categories = 0\n",
    "for key in the_level.keys():\n",
    "    k_categories += len(the_level[key].keys())\n",
    "   \n",
    "    \n",
    "print('processed {} categories in all'.format(k_categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling\n",
    "For each data point, I assigned the deepest class node in the tree to which it belonged as its label. For example, an item named \"bacon ends\" belonged to both classes [Food], [Food, Meats] and [Food, Meats, Pork]. For such an item, I assigned it is label as \"Pork.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "null_examples = df['label'].isnull()\n",
    "df.ix[df[null_examples].index, 'label'] = df.loc[:, 'level_4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "null_examples = df['label'].isnull()\n",
    "df.ix[df[null_examples].index, 'label'] = df.loc[:, 'level_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "null_examples = df['label'].isnull()\n",
    "df.ix[df[null_examples].index, 'label'] = df.loc[:, 'level_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "null_examples = df['label'].isnull()\n",
    "df.ix[df[null_examples].index, 'label'] = df.loc[:, 'level_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "null_examples_ = df['label'].isnull()\n",
    "df.ix[df[null_examples_].index, 'label'] = df.loc[:, 'level_0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the *k* training sets for the *k* categories for each level in the tree   \n",
    "The aim of moving to a category-based solution is to embed knowledge of the taxonomy into classifiers. To do this, I had to figure out how to get positive and negative samples for each category. For every category node, I decided that itself, as well as all its descendants, were *positive* samples for that class. All other nodes that were neither the categories ancestor(s) or itself were set as *negative* samples.  The figure below gives a visual explanation of selecting category training set. I did this for each category node in the tree that had enough data.\n",
    "<img src=\"images/hc_5.png\" alt=\"Drawing\" style=\"width: 450px;\"/>\n",
    "\n",
    "One of the limitations of this method is that most nodes have un-balanced classes. Some more severe than other, especially as you go further down in the tree. There are some classes whose ratio of positive signals is as small as 0.02%. The more granular that subclasses get, the harder it is to classify them. The good news is that one can decide to focus on a few of these classes and use synthetic methods to rebalance the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a dict of positive and negative samples for the categories\n",
    "category_samples = {}\n",
    "category_positive_samples = {}\n",
    "category_negative_samples= {}\n",
    "category_test_samples = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 1: Food\n",
      "processing 2: Supplies\n",
      "processing 3: Beverages\n",
      "processing 4: Other\n",
      "processing 495: Grocery\n",
      "processing 499: Protein\n",
      "Finished Processing\n"
     ]
    }
   ],
   "source": [
    "level = 'level_0'\n",
    "for key in the_level[level].keys():\n",
    "    find_name = df_catergory_lookup.category_id == key\n",
    "    name = df_catergory_lookup[find_name].category_name\n",
    "    the_index = name.index[0]\n",
    "    \n",
    "\n",
    "    print (\"processing {0}: {1}\".format(key, name.ix[the_index]))\n",
    "    # create POSITIVE training set\n",
    "    \n",
    "    key_df = df[level] == key\n",
    "    null_examples = df['level_1'].isnull()\n",
    "    positive_examples = df['level_1'].notnull()\n",
    "    \n",
    "\n",
    "    # Create training set \n",
    "    try:\n",
    "        category_samples[key] = df[key_df & null_examples]\n",
    "        category_samples[key] = category_samples[key].append(df[key_df & positive_examples])\n",
    "        num_test = (len(category_samples[key]) // 10) * 2\n",
    "        category_positive_samples[key] = category_samples[key][num_test:]\n",
    "        category_test_samples[key] = category_samples[key][:num_test]        \n",
    "\n",
    "    except:\n",
    "        print(\"This category didn't generate any feature set {0}\".format(key))\n",
    "        continue\n",
    "        \n",
    "    # create NEGATIVE training set\n",
    "    level_0_item = df['level_0'] != key\n",
    "    \n",
    "    # Create training set \n",
    "    try:\n",
    "        category_negative_samples[key] = df[level_0_item]\n",
    "        \n",
    "    except:\n",
    "        print(\"This category didn't generate any label set {0}\".format(key))\n",
    "        continue\n",
    "        \n",
    "    \n",
    "    \n",
    "print(\"Finished Processing\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 1281: SF Checkout Bag Fee\n",
      "processing 130: Cream\n",
      "processing 375: Sausages\n",
      "processing 132: Tomatoes\n",
      "processing 134: Peppers\n",
      "processing 391: Salt\n",
      "processing 392: Disposables & Packaging Supplies\n",
      "processing 267: Glasses\n",
      "processing 258: Keg Deposit\n",
      "processing 143: Tortillas\n",
      "processing 16: Beers\n",
      "processing 145: Cucumbers\n",
      "processing 18: Wines\n",
      "processing 531: Condiments\n",
      "processing 535: Fruits\n",
      "processing 152: Flour & Starch\n",
      "processing 537: Herbs\n",
      "processing 26: Juices\n",
      "processing 157: Garlic\n",
      "processing 1030: Seeds\n",
      "processing 40: Teas\n",
      "processing 44: Beans\n",
      "processing 482: Linens\n",
      "processing 177: Onions\n",
      "processing 51: Fuel & Freight/Delivery\n",
      "processing 1077: Breads\n",
      "processing 54: Liquor\n",
      "processing 55: Oils\n",
      "processing 56: Pork\n",
      "processing 1730: Leaves\n",
      "processing 325: Eggs\n",
      "processing 70: Beef\n",
      "processing 456: Sodas\n",
      "processing 329: Poultry\n",
      "processing 74: Fish\n",
      "processing 331: Coffee\n",
      "processing 76: Spices\n",
      "processing 226: Vinegars\n",
      "processing 78: Cheese\n",
      "processing 464: Canned\n",
      "processing 1272: Squash\n",
      "processing 84: Lettuce\n",
      "processing 1365: Spreads & Pastes\n",
      "processing 86: Asparagus\n",
      "processing 87: Syrup\n",
      "processing 216: Radishes\n",
      "processing 89: Kale\n",
      "processing 986: Cleaning & Janitorial\n",
      "processing 1039: Nuts & Grains\n",
      "processing 98: Beets\n",
      "processing 228: Sugar\n",
      "processing 1041: Medical Supplies\n",
      "processing 105: Lamb\n",
      "processing 235: Chocolate\n",
      "processing 109: Broccoli\n",
      "processing 111: Mushrooms\n",
      "processing 1960: Glass Cleaner\n",
      "processing 115: Butter \n",
      "processing 118: Potatoes\n",
      "processing 119: Cabbages\n",
      "processing 332: Pasta\n",
      "processing 1078: Shellfish\n",
      "processing 124: Milk\n",
      "processing 125: Carrots\n",
      "processing 126: Cauliflower\n",
      "Finished Processing\n"
     ]
    }
   ],
   "source": [
    "level = 'level_2'\n",
    "for key in the_level[level].keys():\n",
    "    find_name = df_catergory_lookup.category_id == key\n",
    "    name = df_catergory_lookup[find_name].category_name\n",
    "    the_index = name.index[0]\n",
    "\n",
    "    print (\"processing {0}: {1}\".format(key, name.ix[the_index]))\n",
    "    # create POSITIVE training set\n",
    "    \n",
    "    key_df = df[level] == key\n",
    "    null_examples = df['level_3'].isnull()\n",
    "    positive_examples = df['level_3'].notnull()\n",
    "    \n",
    "\n",
    "    # Create training set \n",
    "    try:\n",
    "        category_samples[key] = df[key_df & null_examples]\n",
    "        category_samples[key] = category_samples[key].append(df[key_df & positive_examples])\n",
    "        num_test = (len(category_samples[key]) // 10) * 2\n",
    "        category_positive_samples[key] = category_samples[key][num_test:]\n",
    "        category_test_samples[key] = category_samples[key][:num_test]        \n",
    "    \n",
    "    except:\n",
    "        print(\"This category didn't generate any feature set {0}\".format(key))\n",
    "        continue\n",
    "        \n",
    "    # create NEGATIVE training set\n",
    "    find_level_0 = df[level] == key\n",
    "    name_level_0 = df[find_level_0].level_0\n",
    "    the_index_level_0 = name_level_0.index[0]\n",
    "    \n",
    "    find_level_1 = df[level] == key\n",
    "    name_level_1 = df[find_level_1].level_1\n",
    "    the_index_level_1 = name_level_1.index[0]\n",
    "    \n",
    "    level_0_item = df['level_0'] != name_level_0.ix[the_index_level_0]\n",
    "    level_1_item = df['level_1'] != name_level_1.ix[the_index_level_1]\n",
    "    level_2_item = df['level_2'] != key\n",
    "    \n",
    "    # Create training set \n",
    "    try:\n",
    "        category_negative_samples[key] = df[level_0_item & level_1_item & level_2_item]\n",
    "        \n",
    "    except:\n",
    "        print(\"This category didn't generate any label set {0}\".format(key))\n",
    "        continue    \n",
    "    \n",
    "print(\"Finished Processing\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 387: Oysters\n",
      "processing 136: Cilantro\n",
      "processing 1033: Ground Black Pepper\n",
      "processing 1802: Baby Carrot\n",
      "processing 1239: Lids\n",
      "processing 275: Containers\n",
      "processing 148: Olives\n",
      "processing 25: Vodka\n",
      "processing 282: Uniforms\n",
      "processing 286: Bags\n",
      "processing 32: Gin\n",
      "processing 289: Cups\n",
      "processing 164: Mints\n",
      "processing 39: Tequila\n",
      "processing 176: Oranges\n",
      "processing 1160: Red Wines\n",
      "processing 50: Iced Tea\n",
      "processing 52: Whiskey\n",
      "processing 437: Bacon\n",
      "processing 1209: Gloves\n",
      "processing 319: Red Onions\n",
      "processing 963: Mozzarella\n",
      "processing 69: Chicken\n",
      "processing 970: Extra Virgin Olive Oil\n",
      "processing 1200: Shrimp\n",
      "processing 1358: Rice\n",
      "processing 80: Apples\n",
      "processing 355: Duck\n",
      "processing 980: Dried Fruits\n",
      "processing 983: Cheddar\n",
      "processing 88: Avocados\n",
      "processing 1113: Kegs\n",
      "processing 95: Basil\n",
      "processing 99: Bell Peppers\n",
      "processing 229: Sauces\n",
      "processing 998: White Wines\n",
      "processing 363: Rum\n",
      "processing 403: Purees\n",
      "processing 247: Zucchini Squash\n",
      "processing 1019: Salmon\n",
      "processing 1148: Liqueur\n",
      "processing 1021: Tuna\n",
      "Finished Processing\n"
     ]
    }
   ],
   "source": [
    "level = 'level_3'\n",
    "for key in the_level[level].keys():\n",
    "    find_name = df_catergory_lookup.category_id == key\n",
    "    name = df_catergory_lookup[find_name].category_name\n",
    "    the_index = name.index[0]\n",
    "\n",
    "    print (\"processing {0}: {1}\".format(key, name.ix[the_index]))\n",
    "    # create POSITIVE training set\n",
    "    \n",
    "    key_df = df[level] == key\n",
    "    null_examples = df['level_4'].isnull()\n",
    "    positive_examples = df['level_4'].notnull()\n",
    "    \n",
    "\n",
    "    # Create training set \n",
    "    try:\n",
    "        category_samples[key] = df[key_df & null_examples]\n",
    "        category_samples[key] = category_samples[key].append(df[key_df & positive_examples])\n",
    "        num_test = (len(category_samples[key]) // 10) * 2\n",
    "        category_positive_samples[key] = category_samples[key][num_test:]\n",
    "        category_test_samples[key] = category_samples[key][:num_test]        \n",
    "  \n",
    "    except:\n",
    "        print(\"This category didn't generate any feature set {0}\".format(key))\n",
    "        continue\n",
    "        \n",
    "    # create NEGATIVE training set\n",
    "    find_level_0 = df[level] == key\n",
    "    name_level_0 = df[find_level_0].level_0\n",
    "    the_index_level_0 = name_level_0.index[0]\n",
    "    \n",
    "    find_level_1 = df[level] == key\n",
    "    name_level_1 = df[find_level_1].level_1\n",
    "    the_index_level_1 = name_level_1.index[0]\n",
    "    \n",
    "    find_level_2 = df[level] == key\n",
    "    name_level_2 = df[find_level_2].level_1\n",
    "    the_index_level_2 = name_level_2.index[0]\n",
    "    \n",
    "    level_0_item = df['level_0'] != name_level_0.ix[the_index_level_0]\n",
    "    level_1_item = df['level_1'] != name_level_1.ix[the_index_level_1]\n",
    "    level_2_item = df['level_2'] != name_level_2.ix[the_index_level_2]\n",
    "    level_3_item = df['level_3'] != key\n",
    "    \n",
    "    # Create training set \n",
    "    try:\n",
    "        category_negative_samples[key] = df[level_0_item & level_1_item & level_2_item & level_3_item]\n",
    "        \n",
    "    except:\n",
    "        print(\"This category didn't generate any label set {0}\".format(key))\n",
    "        continue    \n",
    "    \n",
    "print(\"Finished Processing\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 35: Bourbon\n",
      "processing 999: Cabernet Sauvignon\n",
      "processing 1064: Chardonnay\n",
      "processing 1002: Sauvignon Blanc\n",
      "processing 1164: Brut\n",
      "processing 49: Rye\n",
      "processing 21: Rose Wine\n",
      "processing 1015: Chicken Breast\n",
      "processing 989: Pinot Noir\n",
      "Finished Processing\n"
     ]
    }
   ],
   "source": [
    "level = 'level_4'\n",
    "for key in the_level[level].keys():\n",
    "    find_name = df_catergory_lookup.category_id == key\n",
    "    name = df_catergory_lookup[find_name].category_name\n",
    "    the_index = name.index[0]\n",
    "\n",
    "    print (\"processing {0}: {1}\".format(key, name.ix[the_index]))\n",
    "    # create POSITIVE training set\n",
    "    \n",
    "    key_df = df[level] == key\n",
    "    null_examples = df['level_5'].isnull()\n",
    "    positive_examples = df['level_5'].notnull()\n",
    "    \n",
    "\n",
    "    # Create training set \n",
    "    try:\n",
    "        category_samples[key] = df[key_df & null_examples]\n",
    "        category_samples[key] = category_samples[key].append(df[key_df & positive_examples])\n",
    "        num_test = (len(category_samples[key]) // 10) * 2\n",
    "        category_positive_samples[key] = category_samples[key][num_test:]\n",
    "        category_test_samples[key] = category_samples[key][:num_test]        \n",
    "\n",
    "    except:\n",
    "        print(\"This category didn't generate any feature set {0}\".format(key))\n",
    "        continue\n",
    "        \n",
    "    # create NEGATIVE training set\n",
    "    find_level_0 = df[level] == key\n",
    "    name_level_0 = df[find_level_0].level_0\n",
    "    the_index_level_0 = name_level_0.index[0]\n",
    "    \n",
    "    find_level_1 = df[level] == key\n",
    "    name_level_1 = df[find_level_1].level_1\n",
    "    the_index_level_1 = name_level_1.index[0]\n",
    "    \n",
    "    find_level_2 = df[level] == key\n",
    "    name_level_2 = df[find_level_2].level_1\n",
    "    the_index_level_2 = name_level_2.index[0]\n",
    "    \n",
    "    find_level_3 = df[level] == key\n",
    "    name_level_3 = df[find_level_3].level_1\n",
    "    the_index_level_3 = name_level_3.index[0]\n",
    "    \n",
    "    level_0_item = df['level_0'] != name_level_0.ix[the_index_level_0]\n",
    "    level_1_item = df['level_1'] != name_level_1.ix[the_index_level_1]\n",
    "    level_2_item = df['level_2'] != name_level_2.ix[the_index_level_2]\n",
    "    level_3_item = df['level_3'] != name_level_3.ix[the_index_level_3]\n",
    "    level_4_item = df['level_4'] != key\n",
    "    \n",
    "    # Create training set \n",
    "    try:\n",
    "        category_negative_samples[key] = df[level_0_item & level_1_item & level_2_item & level_3_item & level_4_item]\n",
    "        \n",
    "    except:\n",
    "        print(\"This category didn't generate any label set {0}\".format(key))\n",
    "        continue    \n",
    "    \n",
    "print(\"Finished Processing\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "With the training sets done, I vectorized them using a mean embedding vectorizer. \n",
    "\n",
    "### Classifier Selection\n",
    "Based on how the positive and negative samples of the categories were generated, i.e., encoding the category hierarchy into the learning set, I inferred that the classes for each category would be linearly seperable. So for classifier selection, I experimented with an SVM, and AdaBoost and Random Forest. The SVM took too long to run for $K$ = 5 fold cross-validation. While AdaBoost ran faster, the score was lower than that of the Random Forest. Further, I could parallelize the Random Forest which significantly improved the running time.\n",
    "\n",
    "For each category, I created an Random Forest binary classifier. I train each classifier using a $K=5$ fold cross-validation scheme. I fitted the resulting classifier and retrieved the predicted probability for each data point in the dataset, which resulted in *$K$* vectors for *$K$* categories. For each classifier, I calibrated its predicted probability by using an *isotonic* calibration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process item label names using Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "# Only use item labels as input into word2vec embeddings\n",
    "# train word2vec on all the item_labels \n",
    "\n",
    "w2v_model = Word2Vec(df['item_labels'], size=750, window=5, min_count=5, workers=4)\n",
    "w2v = dict(zip(w2v_model.index2word, w2v_model.syn0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w2v_model.init_sims(replace=True) #to trim unneeded model memory = use (much) less RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train *k* classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from time import time, gmtime, strftime\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "seed = 342 # For reproducability\n",
    "Ada_w2v =  Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v)), \n",
    "                     (\"Ada\", AdaBoostClassifier(n_estimators=10))])\n",
    "\n",
    "RF_w2v = Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v)), \n",
    "                     (\"RF\", RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=seed))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    scores = joblib.load(dataPath+'data/df_category_scores.pkl')\n",
    "except:\n",
    "    scores = {}\n",
    "try:\n",
    "    pred_proba = joblib.load(dataPath+'data/df_category_pred_proba.pkl')\n",
    "except:\n",
    "    pred_proba = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.002s\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "\n",
    "\n",
    "    \n",
    "try:\n",
    "    if (pred_proba <> {}) and (scores <> {}) :\n",
    "        target = open(dataPath+'data/train_log_'+strftime(\"%d_%b_%Y_%H_%M_%S\", gmtime())+'.txt', 'w')\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "        target.close()\n",
    "    \n",
    "except:\n",
    "    target = open(dataPath+'data/train_log_'+strftime(\"%d_%b_%Y_%H_%M_%S\", gmtime())+'.txt', 'w')\n",
    "    for key in category_positive_samples:\n",
    "        if str(key) in scores:\n",
    "            continue\n",
    "        if key in scores:\n",
    "            continue\n",
    "        else:\n",
    "            find_name = df_catergory_lookup.category_id == key\n",
    "            name = df_catergory_lookup[find_name].category_name\n",
    "            the_index = name.index[0]\n",
    "\n",
    "            print (\"processing {0}: {1}\".format(key, name.ix[the_index]))\n",
    "            target.write(\"processing {0}: {1}\\n\".format(key, name.ix[the_index]))\n",
    "            category_positive_samples[key].loc[:,('is_category')] = 1\n",
    "            category_negative_samples[key].loc[:,('is_category')] = 0\n",
    "\n",
    "            data = category_positive_samples[key].append(category_negative_samples[key], ignore_index=True)\n",
    "            X = data['item_labels']\n",
    "            y = data['is_category']\n",
    "            target.write('{:5.3f}% positive samples\\n'.format(len(category_positive_samples[key])/len(X)))\n",
    "            target.write(\"Number of datapoints in set {}\\n\".format(len(X)))\n",
    "\n",
    "            scores[key] = cross_val_score(RF_w2v,  X, y, cv=5, scoring='f1').mean()\n",
    "            RF_w2v.fit(X, y)\n",
    "            pred_proba[key] = RF_w2v.predict_proba(X)\n",
    "\n",
    "            # persist model\n",
    "            joblib.dump(RF_w2v, dataPath+'data/category_classifier/catg_'+str(key)+'.pkl')    \n",
    "            print('Finished training category {0}: {1}\\n'.format(key, name.ix[the_index]))\n",
    "            target.write('Finished training category {0}: {1}\\n\\n'.format(key, name.ix[the_index]))\n",
    "\n",
    "    target.close()    \n",
    "    joblib.dump(scores, dataPath+'data/df_category_scores.pkl')\n",
    "    joblib.dump(pred_proba,dataPath+'data/df_category_pred_proba.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get *k* vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key in category_positive_samples:\n",
    "    if 'catg_'+str(key) in df:\n",
    "        continue\n",
    "    else:\n",
    "        df_ = category_positiv_samples[key].append(category_negative_samples[key])\n",
    "        scores_ = pd.DataFrame(pred_proba[key], columns = ['Neg','catg_'+str(key)])\n",
    "        scores_.drop(['Neg'], axis = 1, inplace = True)\n",
    "        scores_ = scores_.set_index(df_.index)\n",
    "        df.ix[df_.index, 'catg_'+str(key)] = scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write vectors to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_pickle('data/df_data_vectors.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model Architecture\n",
    "<figure>\n",
    "  <!---img src=\"images/hc_4.png\" style=\"width: 450px;\"--->\n",
    "</figure>\n",
    "After training the *k=141* classifiers, I extracted the *k* vectors; I carefully combined them with the engineered features, and the other raw metadata taking care to ensure that I assigned the right probabilities to the right data points in my training set. I feed these features and labels into a multi-label, multi-class Random Forest classifier. \n",
    "\n",
    "I took 75% of the data for training and 25% for testing. Once again I do a 5 fold cross-validation scheme, fit the final classifier, and retrieve the predicted class.\n",
    "\n",
    "\n",
    "### Get Data for Final Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Get Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    data = joblib.load(dataPath+'data/data_test.pkl') \n",
    "except:\n",
    "    # Merge all test datasets\n",
    "    data = pd.DataFrame()\n",
    "    for key in category_test_samples:\n",
    "        data = data.append(category_test_samples[key], ignore_index=False)\n",
    "    X_test_0 = data['item_labels']\n",
    "\n",
    "    for keys in category_positive_samples:\n",
    "        catg = 'catg_'+str(key)\n",
    "        clf = joblib.load(dataPath+'data/category_classifier/'+catg+'.pkl')\n",
    "        answer = clf.predict_proba(X_test_0)\n",
    "        scores_ = pd.DataFrame(answer, columns = ['Neg',catg])\n",
    "        scores_.drop(['Neg'], axis = 1, inplace = True)\n",
    "        data.ix[:, catg] = scores_\n",
    "        joblib.dump(clf, dataPath+'data/category_classifier/'+catg+'.pkl')\n",
    "\n",
    "    joblib.dump(data, dataPath+'data/data_test.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Drop duplicates\n",
    "data = data.drop_duplicates(['item_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Make sure you are testing on datapoints whose classifiers have been trained\n",
    "label_mask_test = data[['label']].isin([key for key in scores.keys()]).all(axis=1)\n",
    "data = data.ix[label_mask_test, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge all training datasets\n",
    "\n",
    "the_df = pd.DataFrame()\n",
    "for key in category_positive_samples:\n",
    "    the_df = the_df.append(category_positive_samples[key], ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "the_df = the_df.drop_duplicates(['item_id'], keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop rows that have the overlap between training and testing data from the training data\n",
    "\n",
    "test_item_id = [x for x in data.item_id]\n",
    "training_item_id = [x for x in the_df.item_id]\n",
    "overlap = set(training_item_id).intersection(test_item_id)\n",
    "mask = the_df[['item_id']].isin(list(overlap)).all(axis=1)\n",
    "overlap_index = the_df.ix[mask].index \n",
    "the_df.drop(the_df.ix[mask].index, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on present labels\n",
    "For both training and testing, we can only train and test on datapoints whose category labels we have trained classifiers for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_mask_train = the_df[['label']].isin([key for key in scores.keys()]).all(axis=1)\n",
    "the_df = the_df.ix[label_mask_train, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set training and  testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Drop categories that we didn't train for in this implementation\n",
    "col_name = ['label', u'item_id', u'price_stddev', u'primary_unit', u'price_mean',\n",
    " u'vendor_id', u'branch_lenght', u'item_name_match'] + ['catg_'+str(key) for key in scores.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = the_df[col_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = X[X['label'].notnull()]\n",
    "y = X['label']\n",
    "X = X.fillna(0)\n",
    "X.drop(['label'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "category_mask = data[['label']].isin(y.unique()).all(axis=1)\n",
    "X_test = data[category_mask]\n",
    "X_test = X_test[col_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test = X_test['label']\n",
    "X_test = X_test.fillna(0)\n",
    "X_test.drop(['label'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>price_stddev</th>\n",
       "      <th>primary_unit</th>\n",
       "      <th>price_mean</th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>branch_lenght</th>\n",
       "      <th>item_name_match</th>\n",
       "      <th>catg_1</th>\n",
       "      <th>catg_2</th>\n",
       "      <th>catg_3</th>\n",
       "      <th>...</th>\n",
       "      <th>catg_989</th>\n",
       "      <th>catg_482</th>\n",
       "      <th>catg_998</th>\n",
       "      <th>catg_999</th>\n",
       "      <th>catg_1002</th>\n",
       "      <th>catg_495</th>\n",
       "      <th>catg_499</th>\n",
       "      <th>catg_1015</th>\n",
       "      <th>catg_1019</th>\n",
       "      <th>catg_1021</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55507</th>\n",
       "      <td>349603</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>8738</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55508</th>\n",
       "      <td>352946</td>\n",
       "      <td>5.630275</td>\n",
       "      <td>3</td>\n",
       "      <td>31.800000</td>\n",
       "      <td>7280</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55509</th>\n",
       "      <td>355295</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>13.950000</td>\n",
       "      <td>9724</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55510</th>\n",
       "      <td>360428</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>1146</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55511</th>\n",
       "      <td>362518</td>\n",
       "      <td>0.638381</td>\n",
       "      <td>3</td>\n",
       "      <td>9.128294</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       item_id  price_stddev  primary_unit  price_mean  vendor_id  \\\n",
       "55507   349603      0.000000             3   52.000000       8738   \n",
       "55508   352946      5.630275             3   31.800000       7280   \n",
       "55509   355295      0.000000             3   13.950000       9724   \n",
       "55510   360428      0.000000             3    7.500000       1146   \n",
       "55511   362518      0.638381             3    9.128294         22   \n",
       "\n",
       "       branch_lenght  item_name_match  catg_1  catg_2  catg_3    ...      \\\n",
       "55507              4              1.0     0.9     0.0     0.0    ...       \n",
       "55508              4              1.0     1.0     0.0     0.0    ...       \n",
       "55509              4              0.0     1.0     0.0     0.0    ...       \n",
       "55510              4              1.0     1.0     0.0     0.0    ...       \n",
       "55511              4              0.0     1.0     0.0     0.0    ...       \n",
       "\n",
       "       catg_989  catg_482  catg_998  catg_999  catg_1002  catg_495  catg_499  \\\n",
       "55507       0.0       0.0       0.0       0.0        0.0       0.0       0.0   \n",
       "55508       0.0       0.0       0.0       0.0        0.0       0.0       0.0   \n",
       "55509       0.0       0.0       0.0       0.0        0.0       0.0       0.0   \n",
       "55510       0.0       0.0       0.0       0.0        0.0       0.0       0.1   \n",
       "55511       0.0       0.0       0.0       0.0        0.0       0.0       0.0   \n",
       "\n",
       "       catg_1015  catg_1019  catg_1021  \n",
       "55507        0.0        0.2        0.0  \n",
       "55508        0.0        0.2        0.0  \n",
       "55509        0.0        0.0        0.0  \n",
       "55510        0.2        0.0        0.0  \n",
       "55511        0.0        0.3        0.0  \n",
       "\n",
       "[5 rows x 129 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>price_stddev</th>\n",
       "      <th>primary_unit</th>\n",
       "      <th>price_mean</th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>branch_lenght</th>\n",
       "      <th>item_name_match</th>\n",
       "      <th>catg_1</th>\n",
       "      <th>catg_2</th>\n",
       "      <th>catg_3</th>\n",
       "      <th>...</th>\n",
       "      <th>catg_989</th>\n",
       "      <th>catg_482</th>\n",
       "      <th>catg_998</th>\n",
       "      <th>catg_999</th>\n",
       "      <th>catg_1002</th>\n",
       "      <th>catg_495</th>\n",
       "      <th>catg_499</th>\n",
       "      <th>catg_1015</th>\n",
       "      <th>catg_1019</th>\n",
       "      <th>catg_1021</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>106620</th>\n",
       "      <td>553484</td>\n",
       "      <td>0.6</td>\n",
       "      <td>7</td>\n",
       "      <td>31.29</td>\n",
       "      <td>1475</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.206667</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.685000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106900</th>\n",
       "      <td>562929</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>54.55</td>\n",
       "      <td>1244</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106967</th>\n",
       "      <td>565698</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>35.90</td>\n",
       "      <td>1745</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.936667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106971</th>\n",
       "      <td>565756</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>140.95</td>\n",
       "      <td>1475</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.903333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107166</th>\n",
       "      <td>573430</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>198.50</td>\n",
       "      <td>821</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.834000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        item_id  price_stddev  primary_unit  price_mean  vendor_id  \\\n",
       "106620   553484           0.6             7       31.29       1475   \n",
       "106900   562929           0.0             4       54.55       1244   \n",
       "106967   565698           0.0             1       35.90       1745   \n",
       "106971   565756           0.0             7      140.95       1475   \n",
       "107166   573430           0.0             1      198.50        821   \n",
       "\n",
       "        branch_lenght  item_name_match    catg_1  catg_2    catg_3    ...      \\\n",
       "106620              3              0.0  0.206667    0.01  0.685000    ...       \n",
       "106900              3              0.0  0.060000    0.01  0.880000    ...       \n",
       "106967              3              0.0  0.030000    0.01  0.936667    ...       \n",
       "106971              3              0.0  0.020000    0.02  0.903333    ...       \n",
       "107166              3              0.0  0.090000    0.00  0.834000    ...       \n",
       "\n",
       "        catg_989  catg_482  catg_998  catg_999  catg_1002  catg_495  catg_499  \\\n",
       "106620       0.0       0.0       0.0       0.0        0.0       0.0       0.0   \n",
       "106900       0.0       0.0       0.0       0.0        0.0       0.0       0.0   \n",
       "106967       0.0       0.0       0.0       0.0        0.0       0.0       0.0   \n",
       "106971       0.0       0.0       0.0       0.0        0.0       0.0       0.0   \n",
       "107166       0.0       0.0       0.0       0.0        0.0       0.0       0.0   \n",
       "\n",
       "        catg_1015  catg_1019  catg_1021  \n",
       "106620        0.0        0.0        0.0  \n",
       "106900        0.0        0.0        0.0  \n",
       "106967        0.0        0.0        0.0  \n",
       "106971        0.0        0.0        0.0  \n",
       "107166        0.0        0.0        0.0  \n",
       "\n",
       "[5 rows x 129 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train final classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17015, 129)\n",
      "(9575, 129)\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "          'DecisionTree': DecisionTreeClassifier(random_state=seed),\n",
    "          'RandomForest': RandomForestClassifier(n_estimators=10, n_jobs=-1, random_state=seed),\n",
    "         }\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X_transform = scaler.fit_transform(X)\n",
    "X_test_transform  = scaler.fit_transform(X_test)\n",
    "\n",
    "print(X_transform.shape)\n",
    "print(X_test_transform.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFICATION RESULTS OF BASELINE CLASSIFIERS\n",
      "\n",
      "CLASSIFIER           MEAN SCORE %  STD DEV %    TIME   \n",
      "DecisionTree             98.95        0.29         3.28secs\n",
      "RandomForest             99.10        0.19         2.38secs\n"
     ]
    }
   ],
   "source": [
    "print('CLASSIFICATION RESULTS OF BASELINE CLASSIFIERS\\n')\n",
    "print('{:20}{:^15}{:^10}{:^10}'.format('CLASSIFIER', 'MEAN SCORE %', 'STD DEV %', 'TIME'))\n",
    "\n",
    "\n",
    "for clf_name, clf in models.iteritems():\n",
    "    t0 = time()\n",
    "    results = cross_val_score(clf, X_transform, y, cv=5)\n",
    "    t1 = time() - t0\n",
    "    print('{:20}{:^15.2f}{:^10.2f}{:>10.2f}secs'.format(clf_name, results.mean()*100, results.std()*100, t1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:     46.366\n",
      "Recall:   38.332  \n",
      "F Score  33.905  \n",
      "Support:  63.000  \n",
      "Chance:  1.587   \n",
      "182.90secs\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=1000, n_jobs=-1, random_state=seed)\n",
    "calibrated_clf = CalibratedClassifierCV(clf, method='sigmoid', cv=5)\n",
    "calibrated_clf.fit(X_transform, y)\n",
    "final_preds = calibrated_clf.predict(X_test_transform)\n",
    "precision, recall, fbeta_score, support = score(y_test, final_preds)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print (\"Precision: {:10.3f}\\nRecall: {:^10.3f}\\nF Score{:^10.3f}\\nSupport:{:^10.3f}\\nChance:{:^10.3f}\".format(precision.mean()*100, \n",
    "                                                 recall.mean()*100, fbeta_score.mean()*100, len(support), (1/len(support))*100))\n",
    "\n",
    "print(\"{:<5.2f}secs\".format(time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "|Tree Level|mapping|Precision|Recall|Chance|\n",
    "|---|---|---|---|---|\n",
    "|One |92% |94% |90% | 25%|\n",
    "|Two | 66%| 80%| 64%| 7.0%|\n",
    "|Three | 47%|60% |48% | 2.5%|\n",
    "|Four|39% |44% |50% |3.0% |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Let's take a look at the results\n",
    "df_output = pd.DataFrame()\n",
    "df_output['truth'] = y_test\n",
    "df_output['predicted'] = final_preds\n",
    "df_output['predicted_name'] = ''\n",
    "df_output['item_name'] = data['item_name']\n",
    "df_output['mapped_level_0'] = data['mapped_level_0']\n",
    "df_output['level_0'] = data['level_0']\n",
    "df_output['preds_level_0'] = NaN\n",
    "df_output['mapped_level_1'] = data['mapped_level_1']\n",
    "df_output['level_1'] = data['level_1']\n",
    "df_output['preds_level_1'] = NaN\n",
    "df_output['mapped_level_2'] = data['mapped_level_2']\n",
    "df_output['level_2'] = data['level_2']\n",
    "df_output['preds_level_2'] = NaN\n",
    "df_output['mapped_level_3'] = data['mapped_level_3']\n",
    "df_output['level_3'] = data['level_3']\n",
    "df_output['preds_level_3'] = NaN\n",
    "df_output['mapped_level_4'] = data['mapped_level_4']\n",
    "df_output['level_4'] = data['level_4']\n",
    "df_output['preds_level_4'] = NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in list(df_output.index):\n",
    "    # find the a record where label is the predicted value\n",
    "    the_query = df.ix[:, 'label'] == df_output.ix[i, 'predicted']\n",
    "    answer = df[the_query].head(1)[['item_name', 'level_0', 'level_1', 'level_2', 'level_3', 'level_4']]\n",
    "    df_output.ix[i, 'predicted_name'] = answer.item_name.values[0]\n",
    "    df_output.ix[i, 'preds_level_0'] = answer.level_0.values[0]\n",
    "    df_output.ix[i, 'preds_level_1'] = answer.level_1.values[0]\n",
    "    df_output.ix[i, 'preds_level_2'] = answer.level_2.values[0]\n",
    "    df_output.ix[i, 'preds_level_3'] = answer.level_3.values[0]\n",
    "    df_output.ix[i, 'preds_level_4'] = answer.level_4.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (6, 4)\n",
    "df_output['truth'].plot.hist(bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_output = df_output.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## First Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "precision, recall, fbeta_score, support = score(df_output.level_0, df_output.preds_level_0)\n",
    "print (\"Precision: {:10.3f}\\nRecall: {:^10.3f}\\nF Score{:^10.3f}\\nSupport:{:^10}\\nChance:{:^10.3f}\".format(precision.mean()*100, \n",
    "                                                 recall.mean()*100, fbeta_score.mean()*100, len(support), (1/len(support))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Second Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "precision, recall, fbeta_score, support = score(df_output.level_1, df_output.preds_level_1)\n",
    "print (\"Precision: {:10.3f}\\nRecall: {:^10.3f}\\nF Score{:^10.3f}\\nSupport:{:^10}\\nChance:{:^10.3f}\".format(precision.mean()*100, \n",
    "                                                 recall.mean()*100, fbeta_score.mean()*100, len(support), (1/len(support))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Third Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "precision, recall, fbeta_score, support = score(df_output.level_2, df_output.preds_level_2)\n",
    "print (\"Precision: {:10.3f}\\nRecall: {:^10.3f}\\nF Score{:^10.3f}\\nSupport:{:^10}\\nChance:{:^10.3f}\".format(precision.mean()*100, \n",
    "                                                 recall.mean()*100, fbeta_score.mean()*100, len(support), (1/len(support))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Fourth Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "precision, recall, fbeta_score, support = score(df_output.level_3, df_output.preds_level_3)\n",
    "print (\"Precision: {:10.3f}\\nRecall: {:^10.3f}\\nF Score{:^10.3f}\\nSupport:{:^10.3f}\\nChance:{:^10.3f}\".format(precision.mean()*100, \n",
    "                                                 recall.mean()*100, fbeta_score.mean()*100, len(support), (1/len(support))*100))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
